{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/petertseng0517/REG-TZUCHI-Library/blob/main/%E3%80%90REG_b%E3%80%91RAG02_%E6%89%93%E9%80%A0_RAG_%E7%B3%BB%E7%B5%B1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 0. 讀入你打造好的 vector dataset"
      ],
      "metadata": {
        "id": "b_9at2wXPVKN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install gdown"
      ],
      "metadata": {
        "id": "Usv4C3IOPeQc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GDRIVE_PUBLIC_URL = \"https://drive.google.com/file/d/1LANvgJndrJWrbJ_eFr5vmUgUeRtEYAzz/view?usp=sharing\""
      ],
      "metadata": {
        "id": "KjlSvsjjPlff"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown --fuzzy -O faiss_db.zip \"{GDRIVE_PUBLIC_URL}\""
      ],
      "metadata": {
        "id": "cIN_fDuGQDPL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip faiss_db.zip"
      ],
      "metadata": {
        "id": "rhPQD5g0_l5Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. 安裝並引入必要套件"
      ],
      "metadata": {
        "id": "Cc-facvpBkIi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U langchain langchain-community faiss-cpu transformers sentence-transformers huggingface_hub requests==2.32.4\n",
        "!pip -q install \"aisuite[all]\""
      ],
      "metadata": {
        "id": "9JThdfm-CVZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.chat_models import ChatOpenAI\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings"
      ],
      "metadata": {
        "id": "L1zqb7F8BMP3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "import gradio as gr"
      ],
      "metadata": {
        "id": "PTx3Q75QBp_J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. 自訂 E5 embedding 類別"
      ],
      "metadata": {
        "id": "z13eoo6uCnTT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata"
      ],
      "metadata": {
        "id": "SnWbtp6GvXLa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hf_token = userdata.get('HuggingFace')"
      ],
      "metadata": {
        "id": "b1-hZ354vxfo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "login(token=hf_token)"
      ],
      "metadata": {
        "id": "wvNFzV9jv2Ib"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.embeddings import Embeddings\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings # Import HuggingFaceEmbeddings\n",
        "\n",
        "class EmbeddingGemmaEmbeddings(HuggingFaceEmbeddings): # Inherit from HuggingFaceEmbeddings\n",
        "    def __init__(self, **kwargs): # Remove huggingfacehub_api_token parameter\n",
        "        # Use the HuggingFaceEmbeddings initialization for the model\n",
        "        super().__init__(\n",
        "            model_name=\"google/embeddinggemma-300m\",\n",
        "            **kwargs\n",
        "        )\n",
        "        # Remove custom tokenizer and model loading here, as super().__init__ handles it\n",
        "\n",
        "    # Remove custom embed_documents and embed_query methods,\n",
        "    # as the methods from HuggingFaceEmbeddings will be used.\n",
        "    # If custom logic is absolutely needed, it should be implemented carefully."
      ],
      "metadata": {
        "id": "HkmvGTaECfTY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. 載入 `faiss_db`"
      ],
      "metadata": {
        "id": "NkXNMQs5RbNG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls faiss_db"
      ],
      "metadata": {
        "id": "HIp8PEK4eoud"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_model = EmbeddingGemmaEmbeddings()\n",
        "vectorstore = FAISS.load_local(\n",
        "    \"faiss_db\",\n",
        "    embeddings=embedding_model,\n",
        "    allow_dangerous_deserialization=True\n",
        ")"
      ],
      "metadata": {
        "id": "LkELACdWCtpo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 4})"
      ],
      "metadata": {
        "id": "-tlYrXZWw2tL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. 設定好我們要的 LLM"
      ],
      "metadata": {
        "id": "GrHSAsjcRkXF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "如之前, 我們會用 OpenAI API。這裡使用 Groq 服務, 可改成你要的服務。"
      ],
      "metadata": {
        "id": "WWgbNGakRwrp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import aisuite as ai"
      ],
      "metadata": {
        "id": "uIy_haAeQiB0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "api_key = userdata.get('Groq')"
      ],
      "metadata": {
        "id": "Xefdy-lkRtAL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['GROQ_API_KEY']=api_key"
      ],
      "metadata": {
        "id": "K7UiOKuTDD5F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "這裡的模型和 `base_url` 是用 Groq, 如果用其他服務請自行修改。"
      ],
      "metadata": {
        "id": "654I7y52R8yO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = \"groq:openai/gpt-oss-120b\"\n",
        "#base_url=\"https://api.groq.com/openai/v1\""
      ],
      "metadata": {
        "id": "sqWfH90JFFWV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "client = ai.Client()"
      ],
      "metadata": {
        "id": "JnqlH0W9P2-Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. `prompt` 設計"
      ],
      "metadata": {
        "id": "K0egxeawSR41"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt = \"你是慈濟醫院圖書館館員，請根據資料來回應員工的問題。請親切、簡潔並附帶具體建議。請用台灣習慣的中文回應。\"\n",
        "\n",
        "prompt_template = \"\"\"\n",
        "根據下列資料：\n",
        "{retrieved_chunks}\n",
        "\n",
        "回答使用者的問題：{question}\n",
        "\n",
        "請根據資料內容回覆，若資料不足請告訴同仁可以請教圖書館的陳小姐。\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "ZaUnqDpfFop-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. 使用 RAG 來回應\n",
        "\n",
        "搜尋與使用者問題相關的資訊，根據我們的 prompt 樣版去讓 LLM 回應。"
      ],
      "metadata": {
        "id": "qw8azlVESghL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chat_history = []\n",
        "\n",
        "def chat_with_rag(user_input):\n",
        "    global chat_history\n",
        "    # 取回相關資料\n",
        "    docs = retriever.get_relevant_documents(user_input)\n",
        "    retrieved_chunks = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
        "\n",
        "    # 將自定 prompt 套入格式\n",
        "    final_prompt = prompt_template.format(retrieved_chunks=retrieved_chunks, question=user_input)\n",
        "\n",
        "    # 用 AI Suite 呼叫語言模型\n",
        "    response = client.chat.completions.create(\n",
        "    model=model,\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\"role\": \"user\", \"content\": final_prompt},\n",
        "    ]\n",
        "    )\n",
        "    answer = response.choices[0].message.content\n",
        "\n",
        "    chat_history.append((user_input, answer))\n",
        "    return answer"
      ],
      "metadata": {
        "id": "pWfDUb3mD-6X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. 用 Gradio 打造 Web App"
      ],
      "metadata": {
        "id": "5m7E7XmgTJUr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"# 🎓 慈濟醫院圖書館AI館員\")\n",
        "    chatbot = gr.Chatbot()\n",
        "    msg = gr.Textbox(placeholder=\"請輸入你的問題...\")\n",
        "\n",
        "    def respond(message, chat_history_local):\n",
        "        response = chat_with_rag(message)\n",
        "        chat_history_local.append((message, response))\n",
        "        return \"\", chat_history_local\n",
        "\n",
        "    msg.submit(respond, [msg, chatbot], [msg, chatbot])\n",
        "\n",
        "demo.launch(debug=True)"
      ],
      "metadata": {
        "id": "YI5swv4AFa_U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "605b43a9-b969-4504-b77d-ba8c24d8ac0c"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://cf603747e2541c4683.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "d7uMRFduRywj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "14059de0"
      },
      "source": [
        "user_query = \"請問圖書館的開放時間？\"\n",
        "response = chat_with_rag(user_query)\n",
        "print(response)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}